{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/akshayaravi/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4917fb4d224b6ea32446687b0df835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af817f2599284cb1a375394ea4a77d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/akshayaravi/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5SVCCG/imdb_reviews-train.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f844933c364050b87d4f2fc50d454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/akshayaravi/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5SVCCG/imdb_reviews-test.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417e72d0cd4e478db26de98477aa2e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling and writing examples to /Users/akshayaravi/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5SVCCG/imdb_reviews-unsupervised.tfrecord\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e746b88baf8a4f079ea0704c5c6c2f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/akshayaravi/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "imdb,info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=imdb[\"train\"],imdb[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences=[]\n",
    "train_labels=[]\n",
    "test_sentences=[]\n",
    "test_labels=[]\n",
    "for s,l in train_data:\n",
    "    train_sentences.append(str(s.numpy()))\n",
    "    train_labels.append(l.numpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    test_sentences.append(str(s.numpy()))\n",
    "    test_labels.append(l.numpy())\n",
    "    \n",
    "#convert labels to numpy array\n",
    "\n",
    "train_labels= np.array(train_labels)\n",
    "test_labels= np.array(test_labels)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=10000\n",
    "oov_tok=\"<oov_token>\"\n",
    "max_len=120\n",
    "trunc_type=\"post\"\n",
    "embedding_dim=16\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok) \n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_corp= tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "padded_train_sequence = pad_sequences(train_sequences,maxlen=max_len,truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences= tokenizer.texts_to_sequences(test_sentences)\n",
    "padded_test_sequence = pad_sequences(test_sequences,maxlen=max_len,truncating=trunc_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(\n",
    "    vocab_size,embedding_dim,input_length=max_len),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s 132us/sample - loss: 0.5028 - accuracy: 0.7334 - val_loss: 0.3898 - val_accuracy: 0.8238\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3s 115us/sample - loss: 0.2384 - accuracy: 0.9079 - val_loss: 0.4114 - val_accuracy: 0.8202\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 3s 127us/sample - loss: 0.0872 - accuracy: 0.9783 - val_loss: 0.5149 - val_accuracy: 0.8046\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 4s 146us/sample - loss: 0.0204 - accuracy: 0.9978 - val_loss: 0.6075 - val_accuracy: 0.8045\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s 103us/sample - loss: 0.0049 - accuracy: 0.9998 - val_loss: 0.6806 - val_accuracy: 0.8064\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 3s 105us/sample - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.7390 - val_accuracy: 0.8077\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 3s 137us/sample - loss: 8.3549e-04 - accuracy: 1.0000 - val_loss: 0.7911 - val_accuracy: 0.8079\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 3s 115us/sample - loss: 4.6068e-04 - accuracy: 1.0000 - val_loss: 0.8366 - val_accuracy: 0.8076\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 3s 128us/sample - loss: 2.6223e-04 - accuracy: 1.0000 - val_loss: 0.8814 - val_accuracy: 0.8066\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 4s 157us/sample - loss: 1.5935e-04 - accuracy: 1.0000 - val_loss: 0.9244 - val_accuracy: 0.8074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6469ebf50>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded_train_sequence, train_labels, epochs=num_epochs, validation_data=(padded_test_sequence, test_labels))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Embedding(\n",
    "    vocab_size,embedding_dim,input_length=max_len),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(6,activation='relu'),\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 160,109\n",
      "Trainable params: 160,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 3s 129us/sample - loss: 0.5543 - accuracy: 0.7626 - val_loss: 0.4216 - val_accuracy: 0.8184\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 3s 111us/sample - loss: 0.3395 - accuracy: 0.8591 - val_loss: 0.3689 - val_accuracy: 0.8369\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 3s 110us/sample - loss: 0.2751 - accuracy: 0.8886 - val_loss: 0.3726 - val_accuracy: 0.8351\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 3s 112us/sample - loss: 0.2387 - accuracy: 0.9070 - val_loss: 0.3886 - val_accuracy: 0.8319\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 3s 114us/sample - loss: 0.2129 - accuracy: 0.9194 - val_loss: 0.4103 - val_accuracy: 0.8265\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 3s 114us/sample - loss: 0.1917 - accuracy: 0.9288 - val_loss: 0.4386 - val_accuracy: 0.8211\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 3s 113us/sample - loss: 0.1745 - accuracy: 0.9375 - val_loss: 0.4691 - val_accuracy: 0.8165\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 3s 115us/sample - loss: 0.1594 - accuracy: 0.9449 - val_loss: 0.5052 - val_accuracy: 0.8116\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 4s 144us/sample - loss: 0.1460 - accuracy: 0.9502 - val_loss: 0.5375 - val_accuracy: 0.8098\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 3s 114us/sample - loss: 0.1346 - accuracy: 0.9546 - val_loss: 0.5845 - val_accuracy: 0.8028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x63f1e7f50>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded_train_sequence, train_labels, epochs=num_epochs, validation_data=(padded_test_sequence, test_labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence = \"I really think this is amazing. honest.\"\n",
    "#sequence = tokenizer.texts_to_sequences(sentence)\n",
    "#print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_word_index={value:key for key,value in word_corp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = rev_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
